---
title: "Implementing the Delta Method on Correlation Matrices' Covariances"
author:
- Itamar Faran
- Department of Statistics
- The Hebrew University
date: "April, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=12, fig.height=5) 
```

```{r, warning = FALSE, message=FALSE, echo=FALSE}
library(abind)
library(corrplot)
library(knitr)
library(MASS)
library(matrixcalc)
library(tidyverse)

#Force symmetry, as the name implies, forces symmetry on a matrix
force_symmetry <- function(MATR) return((MATR + t(MATR))/2)

vector_to_triangle <- function(VECT){
  m <- length(VECT)
  p <- 0.5*c(1+sqrt(1+8*m), 1-sqrt(1+8*m))
  p <- p[which( (p==round(p))&p==abs(p) )]
  if(length(p)==0) stop("Vect length does not fit size of triangular matrix")
  pelet <- matrix(0, ncol = p, nrow = p)
  pelet[lower.tri(pelet)] <- VECT
  pelet <- pelet+t(pelet)
  diag(pelet) <- 1
  return(pelet)
}

p <- 12

#set.seed(214)
real_sigma <- matrix(runif(p^2, -7, 9), ncol=p) %>% (function(x) t(x)%*%x)

real_corr <- force_symmetry(cov2cor(real_sigma))
is.positive.definite(real_sigma)
is.positive.definite(real_corr)
```

Using the Delta method, we found that:

$n\cdot Cov\left(\hat{\rho}_{ij},\hat{\rho}_{kl}\right)=\frac{\boldsymbol{\rho_{ij}}\boldsymbol{\rho_{kl}}}{2}\left(\rho_{ik}^{2}+\rho_{il}^{2}+\rho_{jk}^{2}+\rho_{jl}^{2}\right)-\boldsymbol{\rho_{ij}}\left(\rho_{ik}\rho_{il}+\rho_{jk}\rho_{jl}\right)-\boldsymbol{\rho_{kl}}\left(\rho_{ik}\cdot\rho_{jk}+\rho_{il}\cdot\rho_{jl}\right)+\left(\rho_{ik}\rho_{jl}+\rho_{il}\rho_{jk}\right)$.

Denote $\widehat{C}_{ij,kl}=\widehat{Cov}\left(\hat{\rho}_{ij},\hat{\rho}_{kl}\right)$ and $\tilde{C}_{ij,kl}=Cov\left(\hat{\rho}_{ij},\hat{\rho}_{kl}\right)$ (the expression above) from now on.

Here is presented the real correlations and variances:

```{r, echo = FALSE}
corrplot(real_corr, method = "circle", addCoef.col = "black",
         diag = FALSE, tl.pos = "d", tl.cex = 1.2,
         number.cex = .8, tl.col = "darkgrey",
         title="(1.1) Correlation matrix", mar=c(0,0,1,0))

temp <- t(data.frame(Variance = round(diag(real_sigma),2)))
colnames(temp) <- 1:p
kable(temp)
```

\newpage

Here I simulate Wishart matrices and compute the empiric correlation matrix:
```{r}
wishart.df <- 100
returns <- 1000

if(wishart.df < p) warning("Lower degrees of freedom than matrix dimension")
results <- rWishart(returns, df = wishart.df, real_sigma)

results2 <- array(dim = dim(results))
for(i in 1:returns) results2[,,i] <- force_symmetry(cov2cor(results[,,i]))
```

'real.cov2' returns the proposed expression for $Cov\left(\hat{\rho}_{ij},\hat{\rho}_{kl}\right)$ (e.g. $\tilde{C}_{ij,kl}$):
```{r}
real.cov2 <- function(i, j, k, l) {
    ((real_corr[i,j]*real_corr[k,l]/2)*
       (real_corr[i,k]^2 + real_corr[i,l]^2 + real_corr[j,k]^2 + real_corr[j,l]^2) -
    real_corr[i,j]*(real_corr[i,k]*real_corr[i,l] + real_corr[j,k]*real_corr[j,l]) - 
    real_corr[k,l]*(real_corr[i,k]*real_corr[j,k] + real_corr[i,l]*real_corr[j,l]) + 
    (real_corr[i,k]*real_corr[j,l] + real_corr[i,l]*real_corr[j,k]))/wishart.df

}
```

'compare_estimate2' returns the proposed expression ($\tilde{C}_{ij,kl}$) versus the empiric covariance, the bias and the bias ratio.
```{r}
compare_estimate2 <- function(i,j,k,l){
  real <- real.cov2(i, j, k, l)
  estimate <- cov(results2[i,j,], results2[k,l,])
  return(list(Index = c(i,j,k,l), Real = real, Estimate = estimate, Bias = real - estimate,
              Bias.Ratio = (real - estimate)/real))
}
```

```{r, echo = FALSE}
m <- 0.5*p*(p-1)
covariance.matrix2 <- matrix(nrow = (m*(m+1)/2), ncol = 10)
colnames(covariance.matrix2) <- c("V-Index_i", "M-Index_R_i", "M-Index_C_i",
                                 "V-Index_j", "M-Index_R_j", "M-Index_C_j",
                                 "Theoretical","Empiric", "Bias", "Bias.Ratio")

v1 <- numeric(0)
v2 <- numeric(0)
for(i in 1:m){
  v1 <- c(v1, rep(i, m-i+1))
  v2 <- c(v2, i:m)
}
covariance.matrix2[,c(1,4)] <- cbind(v1,v2)

v1 <- numeric(0)
v2 <- numeric(0)
for(i in 1:(p-1)){
  v1 <- c(v1, rep(i, p-i))
  v2 <- c(v2, (i+1):p)
}
order_vect <- cbind(v1,v2)

for(i in 1:(m*(m+1)/2)){
  covariance.matrix2[i,2:3] <- order_vect[covariance.matrix2[i,1],]
  covariance.matrix2[i,5:6] <- order_vect[covariance.matrix2[i,4],]
  temp <- compare_estimate2(covariance.matrix2[i,2], covariance.matrix2[i,3],
                            covariance.matrix2[i,5], covariance.matrix2[i,6])
  covariance.matrix2[i, 7] <- temp$Real
  covariance.matrix2[i, 8] <- temp$Estimate
  covariance.matrix2[i, 9] <- temp$Bias
  covariance.matrix2[i, 10] <- temp$Bias.Ratio
}

rm(temp)
covariance.matrix2 <- as.data.frame(covariance.matrix2)
```

This is the data-frame we have:

```{r}
head(covariance.matrix2)[,1:6]
head(covariance.matrix2)[,7:10]
```

Below I present some Regression models to check if there is a linear connection between the following objects:

This is the model $\widehat{C}_{ij,kl}\sim\tilde{C}_{ij,kl}$
```{r, echo=FALSE}
reg_model <- lm(Empiric ~ 0 + Theoretical, data = covariance.matrix2)
obj <- summary(reg_model)
obj
```
The correlation between the empiric and hypothised covariance is `r round(sqrt(obj$r.squared)*100,1)`%, and the slope is `r round(obj$coefficients[1],4)` with a t-value of `r round(obj$coefficients[3],3)`, almost coliding with the $y=x$ line.

This is the model $Bias\sim\tilde{C}_{ij,kl}\iff\left(\tilde{C}_{ij,kl}-\widehat{C}_{ij,kl}\right)\sim\tilde{C}_{ij,kl}$
```{r, echo=FALSE}
reg_model <- lm(Bias ~ 0 + Theoretical, data = covariance.matrix2)
obj <- summary(reg_model)
obj
```
With a slope of `r round(obj$coefficients[1],4)` and a t-value of `r round(obj$coefficients[3],3)` (and an $R^2$ value of `r round(obj$r.squared*100,3)`%) we can conclude that there is no linear relation between the bias and the theoretical covariances.

\newpage

This is the model $Bias\,Ratio\sim\tilde{C}_{ij,kl}\iff\frac{\tilde{C}_{ij,kl}-\widehat{C}_{ij,kl}}{\tilde{C}_{ij,kl}}\sim\tilde{C}_{ij,kl}$

```{r}
reg_model <- lm(Bias.Ratio ~ 0 + Theoretical, data = covariance.matrix2)
obj <- summary(reg_model)
obj
```
Again, a slope of `r round(obj$coefficients[1],4)` and a t-value of `r round(obj$coefficients[3],3)` and an $R^2$ value of `r round(obj$r.squared*100,3)`% we can conclude that there is no linear relation between the bias ratio and the theoretical covariances.

\newpage

Lets have a look at some graphs. 'Unique' is the number of unique values in $\tilde{C}_{ij,kl}$, meaning that for Unique equal to 2 we have $\tilde{C}_{ij,ij}$, for Unique equal to 3 we have $\tilde{C}_{ij,il}$ and for Unique equal to 4 we have $\tilde{C}_{ij,kl}$. The dashed line is the $y=x$ line

```{r, echo=FALSE}
new_dataframe <- covariance.matrix2[,-c(1,4)]

new_dataframe$Unique <- factor(apply(new_dataframe[,1:4], 1, function(x) length(unique(x))),
                               ordered = FALSE)

ggplot(new_dataframe, aes(x = Theoretical, y = Empiric, col = Unique)) + 
    geom_hline(yintercept = 0, col = "grey") + geom_vline(xintercept = 0, col = "grey") + 
    geom_abline(slope = 1, intercept = 0, col = "red", size = 1, linetype = 2) + 
    geom_point(alpha = 0.5) + 
    ggtitle("Empiric ~ Theoretical")
```
The empiric and theoretical values almost collide with the $y=x$ line.


```{r, echo=FALSE}
ggplot(new_dataframe, aes(x = Theoretical, y = Bias, col = Unique)) + 
    geom_hline(yintercept = 0, col = "grey") + geom_vline(xintercept = 0, col = "grey") + 
    geom_point(alpha = 0.5) + 
    ggtitle("Bias ~ Theoretical")
```
It looks like there is no realtion between the Bias and the theoretical values.

```{r, echo=FALSE}
ggplot(new_dataframe, aes(x = Theoretical, y = Bias.Ratio, col = Unique)) + 
    geom_hline(yintercept = 0, col = "grey") + geom_vline(xintercept = 0, col = "grey") + 
    geom_point(alpha = 0.5) + coord_cartesian(ylim = c(-10, 10)) + 
    ggtitle("Bias Ratio ~ Theoretical")
```
Apart from the fact that the bias ratio intends to grow when the theoritcal covariance gets closer to 0, It still looks like there is no realtion between the Bias Ratio and the theoretical values.

```{r, eval=FALSE, echo=FALSE}

plot_type_Emp <- function(type){
  filter(new_dataframe, Unique == type) %>% ggplot(aes(x = Theoretical, y = Empiric)) + 
    geom_hline(yintercept = 0, col = "grey") + geom_vline(xintercept = 0, col = "grey") + 
    geom_abline(slope = 1, intercept = 0, col = "red", size = 1, linetype = 2) + 
    geom_point(col = "blue") + 
    ggtitle(paste("Empiric ~ Theoretical with", type, "unique indexes"))
}

plot_type_Bias <- function(type){
  filter(new_dataframe, Unique == type) %>% ggplot(aes(x = Theoretical, y = Bias)) + 
    geom_hline(yintercept = 0, col = "grey") + geom_vline(xintercept = 0, col = "grey") + 
    geom_abline(slope = 1, intercept = 0, col = "red", size = 1, linetype = 2) + 
    geom_point(col = "blue") + 
    ggtitle(paste("Empiric ~ Theoretical with", type, "unique indexes"))
}

plot_type_BiasRat <- function(type){
  filter(new_dataframe, Unique == type) %>% ggplot(aes(x = Theoretical, y = Bias.Ratio)) + 
    geom_hline(yintercept = 0, col = "grey") + geom_vline(xintercept = 0, col = "grey") + 
    geom_abline(slope = 1, intercept = 0, col = "red", size = 1, linetype = 2) + 
    geom_point(col = "blue") + 
    ggtitle(paste("Empiric ~ Theoretical with", type, "unique indexes"))
}

plot_type_Emp(2)
plot_type_Emp(3)
plot_type_Emp(4)



```
