#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\begin_modules
algorithm2e
theorems-named
eqs-within-sections
theorems-ams-chap-bytype
todonotes
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Title
\end_layout

\begin_layout Author
Itamar Faran, Yuval Benjamini
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Enumerate
scholarly review.
 emphasize that we work on inference
\end_layout

\begin_layout Enumerate
introduce our problem - show that we have more power (?)
\end_layout

\begin_layout Enumerate
\begin_inset Quotes eld
\end_inset

cartoon
\begin_inset Quotes erd
\end_inset

 of parameters, for example corrplot of theta, corrplot of alpha, visualize
 g
\end_layout

\begin_layout Section
Preliminaries
\end_layout

\begin_layout Standard
We begin by discussing distributional properties of correlation matrices
 derived from multivariate time-series data.
 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X$
\end_inset

 be a data matrix measuring the activity of 
\begin_inset Formula $p$
\end_inset

 brain regions (columns) sampled at 
\begin_inset Formula $T$
\end_inset

 time points (rows).
 Assume that 
\begin_inset Formula $X\sim\mathcal{MN}_{T\times p}\left(\boldsymbol{0},\Delta,\Sigma\right)$
\end_inset

, meaning that 
\begin_inset Formula $X$
\end_inset

 follows a multivariate Normal matrix distribution with expected value 
\begin_inset Formula $\boldsymbol{0}$
\end_inset

, row-wise covariance matrix 
\begin_inset Formula $\underset{T\times T}{\Delta}$
\end_inset

, and column-wise covariance matrix 
\begin_inset Formula $\underset{p\times p}{\Sigma}$
\end_inset

.
 Define the empirical covariance operator 
\begin_inset Formula $Cov\left(X\right)=T^{-1}X'X$
\end_inset

, and define the scaling operator on 
\begin_inset Formula $W=Cov\left(X\right)$
\end_inset

 to be 
\begin_inset Formula $scale\left(W\right)=diag\left(W\right)^{-\nicefrac{1}{2}}\cdot W\cdot diag\left(W\right)^{-\nicefrac{1}{2}}$
\end_inset

.
 The empirical correlation operator on 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $X$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 is 
\begin_inset Formula $corr\left(X\right)=scale\left(Cov\left(X\right)\right).$
\end_inset

 When the rows are iid (
\begin_inset Formula $\Delta=I_{T}$
\end_inset

), 
\begin_inset Formula $W=Cov\left(X\right)$
\end_inset

 is the maximum likelihood estimator for 
\begin_inset Formula $\Sigma$
\end_inset

 and 
\begin_inset Formula $T\cdot W$
\end_inset

 follows a 
\begin_inset Formula $Wishart_{p}\left(\Sigma,T\right)$
\end_inset

 distribution.
 Importantly for us, the first two moments of 
\begin_inset Formula $W$
\end_inset

 are the following: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
E\left[W\right] & =\Sigma,\\
T\cdot Cov\left(W_{ij},W_{kl}\right) & =\Sigma_{ij,kl}^{\left(2\right)}=\Sigma_{ik}\Sigma_{jl}+\Sigma_{il}\Sigma_{jk},
\end{align}

\end_inset

as shown for example by [[CITE]].
 However, the elements of empirical correlation matrices have distinctly
 smaller variances because of the scaling.
 The following lemma describes the limiting covariance of values in empirical
 correlation matrices: 
\end_layout

\begin_layout Lemma
The asymptotic moments of empirical correlation matrices: Let 
\begin_inset Formula $R=corr(X)$
\end_inset

 for 
\begin_inset Formula $X\sim\mathcal{MN}_{T\times p}\left(\boldsymbol{0},I_{T},\Sigma\right)$
\end_inset

.
 Denote by 
\begin_inset Formula $\rho_{ij}=\frac{\Sigma_{ij}}{\sqrt{\Sigma_{ii}\Sigma_{jj}}}$
\end_inset

.
 Then
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\underset{T\rightarrow\infty}{\lim}E\left[R\right]=diag\left(\Sigma\right)^{-\nicefrac{1}{2}}\cdot\Sigma\cdot diag\left(\Sigma\right)^{-\nicefrac{1}{2}}\eqqcolon\boldsymbol{\rho},
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\underset{T\rightarrow\infty}{\lim}T\cdot Cov\left(R_{ij},R_{kl}\right)=C_{ij,kl}\left(\boldsymbol{\rho}\right),
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
C_{ij,kl}\left(\boldsymbol{\rho}\right)\coloneqq\frac{\boldsymbol{\rho_{ij}}\boldsymbol{\rho_{kl}}}{2}\left(\rho_{ik}^{2}+\rho_{il}^{2}+\rho_{jk}^{2}+\rho_{jl}^{2}\right)-\boldsymbol{\rho_{ij}}\left(\rho_{ik}\rho_{il}+\rho_{jk}\rho_{jl}\right)-\boldsymbol{\rho_{kl}}\left(\rho_{ik}\cdot\rho_{jk}+\rho_{il}\cdot\rho_{jl}\right)+\left(\rho_{ik}\rho_{jl}+\rho_{il}\rho_{jk}\right).
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The Delta Method on Correlation Matrices
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/itama/Google Drive/Documents/Study/05 Year 2018-2019/74445 Masters Thesis/correlation_glm/main_work/simulations/correlation_delta_method.png
	scale 50

\end_inset


\end_layout

\end_inset

The results rely on the Wishart distribution of 
\begin_inset Formula $W=cov(X)$
\end_inset

 approaching a multi-normal matrix as T increases
\begin_inset Foot
status open

\begin_layout Plain Layout
Since the Wishart distribution is closed under convolution, in the sense
 that if 
\begin_inset Formula $U\sim W_{p}\left(\Sigma,n\right),V\sim W_{p}\left(\Sigma,m\right)$
\end_inset

 then 
\begin_inset Formula $U+V\sim W_{p}\left(\Sigma,m+n\right)$
\end_inset

, The CLT applies and 
\begin_inset Formula $W$
\end_inset

 has a limit distribution of Matrix Normal
\end_layout

\end_inset

, and applying the Delta-method on the coordinates of correlation matrix
 
\begin_inset Formula $R_{ij}=\frac{W_{ij}}{\sqrt{W_{ii}W_{jj}}}$
\end_inset

.
 We are not aware of these results in the literature, and will give a full
 proof in the supplementary.
 
\end_layout

\begin_layout Standard
In fMRI time-series data, the correlation between adjacent samples can be
 large, and hence the gain of information from each observational unit decreases.
 
\begin_inset Formula $W=Cov(X)$
\end_inset

 remains unbiased, but 
\begin_inset Formula $Var(W)$
\end_inset

 and 
\begin_inset Formula $Var(R)$
\end_inset

 will be inflated compared to [[Eq Refs]].
 The following correction, due to Efron [[CITE]], identifies an effective
 degrees of freedom 
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Maybe effective sample size
\end_layout

\end_inset

 
\begin_inset Formula $T_{eff}$
\end_inset

 that can be estimated directly from 
\begin_inset Formula $\Sigma$
\end_inset

.
 We cite the result with no proof.
 
\end_layout

\begin_layout Lemma
The Effective Degrees of Freedom: Let 
\begin_inset Formula $X\sim\mathcal{MN}_{T\times p}\left(\boldsymbol{0},\Delta,\Sigma\right)$
\end_inset

 for general positive-definite 
\begin_inset Formula $\Delta$
\end_inset

.
 Define 
\begin_inset Formula $T_{eff}=\frac{T}{\left(T-1\right)\psi^{2}+1}$
\end_inset

 for 
\begin_inset Formula $\psi(\Sigma)$
\end_inset

 the root-mean-square factor [[Add Equation]].
 Then the covariances of empirical covariance and correlation matrices W
 and R are as before, with the replacement of T with 
\begin_inset Formula $T_{eff}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
T_{eff}(T,\psi)\cdot Cov\left(W_{ij},W_{kl}\right)=\Sigma_{ij,kl}^{\left(2\right)},
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\underset{T\rightarrow\infty}{\lim}T_{eff}(T,\psi)\cdot Cov\left(R_{ij},R_{kl}\right)=C_{ij,kl}\left(\boldsymbol{\rho}\right).
\]

\end_inset


\end_layout

\begin_layout Standard
Finally, we would like to define the vectorizing operator for correlation
 matrices.
 Correlation matrices can be fully characterized using the 
\begin_inset Formula $m=p(p-1)/2$
\end_inset

 upper triangle elements, because they are symmetric with a degenerate diagonal.
 We define 
\begin_inset Formula $vec:\mathbb{M}_{p}\left(\mathbb{R}\right)\mapsto\mathbb{R}^{m}$
\end_inset

 as the vectorization operator with 
\begin_inset Formula $\text{vec}\left(R\right)=\left(R_{12},R_{13,}\cdots,R_{1p},R_{23},\cdots R_{p-1,p}\right)^{T}.$
\end_inset

 We further denote 
\begin_inset Formula $\vec{u}\coloneqq\text{vec}\left(u\right)$
\end_inset

 for the rest of this paper.
 Using the lemmas and definition of 
\begin_inset Formula $vec$
\end_inset

, we can imply that 
\begin_inset Formula $\vec{R}\rightsquigarrow N_{m}\left(\vec{\boldsymbol{\rho}},T_{eff}^{-1}\cdot\Sigma_{\boldsymbol{\rho}}^{\star}\right)$
\end_inset

 (
\begin_inset Formula $\Sigma_{\boldsymbol{\rho}}^{\star}=\Sigma^{\star}\left(\boldsymbol{\rho}\right)$
\end_inset

 as describe in [[Eq Refs]]).
\end_layout

\begin_layout Section
Non-Linear Modeling of Correlation Matrices
\end_layout

\begin_layout Subsection
Model Definition
\end_layout

\begin_layout Standard
Suppose that the collected data consists of two sets of independent samples,
 denoted by 
\begin_inset Formula $\mathcal{D}$
\end_inset

 and 
\begin_inset Formula $\mathcal{H}$
\end_inset

: diagnosed and control subjects, respectively, each holding 
\begin_inset Formula $n_{d}$
\end_inset

 and 
\begin_inset Formula $n_{h}$
\end_inset

 subjects.
 The observational unit of each subject 
\begin_inset Formula $i$
\end_inset

 is a Correlation matrix 
\begin_inset Formula $R_{i}$
\end_inset

 of dimension 
\begin_inset Formula $p\times p$
\end_inset

 obtained from 
\begin_inset Formula $T$
\end_inset

 measurements.
 We will assume that 
\begin_inset Formula $E\left[R_{i}\right]=\Lambda_{h}$
\end_inset

 for 
\begin_inset Formula $i\in\mathcal{H}$
\end_inset

 and 
\begin_inset Formula $\Lambda_{d}$
\end_inset

 for 
\begin_inset Formula $i\in\mathcal{D}$
\end_inset

.
 In order to estimate and infer on differences between these two matrices,
 one can model both with 
\begin_inset Formula $2m$
\end_inset

 parameters and conduct 
\begin_inset Formula $m$
\end_inset

 Hypotheses test.
 Since this procedure involves a great loss of power by the Multiple-Comparisons
 correction that follows, we seek to model these matrices with less parameters.
 Therefore we model 
\begin_inset Formula $\Lambda_{d}$
\end_inset

 as a function of 
\begin_inset Formula $\Lambda_{h}$
\end_inset

 with a limited number of parameters.
 We define 
\begin_inset Formula $\Lambda_{h}=\Theta$
\end_inset

 and 
\begin_inset Formula $\Lambda_{d}=g\left(\Theta,\alpha\right)$
\end_inset

; 
\begin_inset Formula $\Theta$
\end_inset

 can be seen as the baseline for the study, the behavior of control subjects,
 while 
\begin_inset Formula $\alpha\in\mathbb{\mathbb{M}}_{p\times q}\left(\mathbb{R}\right)$
\end_inset

 is the parameterization of the deviance from the baseline of diagnosed
 subjects.
 
\begin_inset Formula $g:\mathbb{M}_{p}\left(\left[-1,1\right]\right)\mapsto\mathbb{M}_{p}\left(\left[-1,1\right]\right)$
\end_inset

, referred to as the link function, is the parameterization of the relation
 between 
\begin_inset Formula $\Lambda_{h}$
\end_inset

 and 
\begin_inset Formula $\Lambda_{d}$
\end_inset

 in terms of 
\begin_inset Formula $\alpha$
\end_inset

.
 There are two properties required from 
\begin_inset Formula $g$
\end_inset

; the first is 
\begin_inset Formula $g$
\end_inset

 being differentiable, and the second is the existence of 
\begin_inset Formula $g^{-1}$
\end_inset

, denoting the 'reverse' of 
\begin_inset Formula $g$
\end_inset

 (not necessarly it's inverse).
 Our working example throughout the paper will be 
\begin_inset Formula $g\left(\Theta,\alpha\right)=\Theta\circ\alpha\alpha^{t}$
\end_inset

 with 
\begin_inset Formula $q=1$
\end_inset

, '
\begin_inset Formula $\circ$
\end_inset

' denoting the Hadamard product: 
\begin_inset Formula $g_{ij}\left(\Theta,\alpha\right)=\Theta_{ij}\cdot\alpha_{i}\alpha_{j}$
\end_inset

, and a reverse function 
\begin_inset Formula $g_{ij}^{-1}\left(R,\alpha\right)=R\cdot\frac{1}{\alpha_{i}\alpha_{j}}$
\end_inset

.
 This link function relies on the assumption that there is a multiplicative
 relation between 
\begin_inset Formula $\Lambda_{h},\Lambda_{d}$
\end_inset

.
 Another option is 
\begin_inset Formula $g_{ij}\left(\Theta,\alpha\right)=\frac{\Theta_{ij}}{1+\alpha_{i}+\alpha_{j}}$
\end_inset

; The methodology presented in the next sections is generalized for all
 link functions.
\end_layout

\begin_layout Subsection
Model Estimation
\end_layout

\begin_layout Standard
Define 
\begin_inset Formula $Y_{i}=vec\left(R_{i}\right)$
\end_inset

, a vectorized correlation matrix of subject 
\begin_inset Formula $i$
\end_inset

.
 By implementing lemma (1-3) and assuming 
\begin_inset Formula $T$
\end_inset

 is relatively large, we assume that 
\begin_inset Formula $Y_{i}\overset{iid}{\sim}N_{m}\left(\vec{g}\left(\Theta,\alpha\right),\Sigma_{g}^{\star}/T\right)$
\end_inset

 for 
\begin_inset Formula $i\in\mathcal{D}$
\end_inset

 where 
\begin_inset Formula $\Sigma_{g}^{\star}=\Sigma^{\star}\left(g\left(\Theta,\alpha\right)\right)$
\end_inset

 is defined as in lemma (), with 
\begin_inset Formula $g\left(\Theta,\alpha\right)$
\end_inset

 replacing 
\begin_inset Formula $\mathbf{\rho}$
\end_inset

.
 This allows us to define a full log-likelihood function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\ell_{full}=\sum_{i\in\mathcal{H}}\ln\phi_{m}\left(Y_{i},\vec{\Theta},\Sigma_{\Theta}^{\star}/T\right)+\sum_{i\in\mathcal{D}}\ln\phi_{m}\left(Y_{i},\vec{g}\left(\Theta,\alpha\right),\Sigma_{g}^{\star}/T\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\phi_{m}$
\end_inset

 is the PDF of an 
\begin_inset Formula $m$
\end_inset

-variate normal RV
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Formula $\left(-2\right)\cdot\ln\phi_{m}\left(x,\mu,\Sigma\right)=Const.+\ln\det\Sigma+\left(x-\mu\right)^{t}\Sigma^{-1}\left(x-\mu\right)$
\end_inset


\end_layout

\end_inset

.
 This optimization might be unstable: in order to maximize the likelihood,
 a possible solution is to take 
\begin_inset Formula $\alpha\rightarrow\infty$
\end_inset

 (since it's inverted in the weighting matrix 
\begin_inset Formula $\Sigma_{g}^{\star}$
\end_inset

 and in the determinant as well).
 In cases where the link function doesn't fit well to the data, this optimizatio
n method will lead to limiting results.
 As a consequence, we look for a solution where the weighting matrix 
\begin_inset Formula $\Sigma^{\star}$
\end_inset

 isn't parameterized with 
\begin_inset Formula $\alpha$
\end_inset

, which leads us to a weighted least squares optimization.
 We use the average correlation matrix to construct an estimate of 
\begin_inset Formula $\Sigma^{\star}$
\end_inset

: we define 
\begin_inset Formula $\bar{R}_{d}=n_{d}^{-1}\sum_{i\in\mathcal{D}}R_{d}$
\end_inset

 and use 
\begin_inset Formula $\bar{\Sigma}_{d}^{\star}=\Sigma^{\star}\left(\bar{R}_{d}\right)$
\end_inset

 as the weighting matrix in the loss function.
 This optimization, that is indefferent to the determinant that appears
 in the Gaussian PDF, yields more stable results than the full log-likelihood
 model.
 Define the following loss functions:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\mathcal{S}^{\left(h\right)} & =\sum_{i\in\mathcal{H}}\left(Y_{i}-\vec{\Theta}\right)^{t}\bar{\Sigma}_{h}^{\star-1}\left(Y_{i}-\vec{\Theta}\right)\\
\mathcal{S}^{\left(d\right)} & =\sum_{i\in\mathcal{D}}\left(Y_{i}-\vec{g}\left(\Theta,\alpha\right)\right)^{t}\bar{\Sigma}_{d}^{\star-1}\left(Y_{i}-\vec{g}\left(\Theta,\alpha\right)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\mathcal{S} & =\mathcal{S}^{\left(h\right)}+\mathcal{S}^{\left(d\right)}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
then our estimates are defined as 
\begin_inset Formula $\hat{\Theta},\hat{\alpha}=\underset{\Theta.\alpha}{\arg\min}\mathcal{S}$
\end_inset

.
 While 
\begin_inset Formula $\alpha$
\end_inset

 has only 
\begin_inset Formula $p$
\end_inset

 parameters, 
\begin_inset Formula $\Theta$
\end_inset

 has 
\begin_inset Formula $m$
\end_inset

 distinct parameters and the joint optimization of 
\begin_inset Formula $\mathcal{S}$
\end_inset

 both on 
\begin_inset Formula $\Theta$
\end_inset

 and 
\begin_inset Formula $\alpha$
\end_inset

 is a computationally-expensive task.
 We address this issue with two measures: First, the optimization is done
 iteratively; Second, since 
\begin_inset Formula $\Theta$
\end_inset

 is not a parameter of interest, it is estimated using a moment estimator.
 Given a set of estimates 
\begin_inset Formula $\hat{\Theta}_{k-1}$
\end_inset

, the estimates of 
\begin_inset Formula $\alpha,\Theta$
\end_inset

 at iteration 
\begin_inset Formula $k$
\end_inset

 are given by: 
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
look for a paper that does iterative MLE optimization and look how he presents
 it (partial? conditional?).
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\hat{\alpha}_{k} & =\underset{\alpha}{\arg\min}\sum_{i\in\mathcal{D}}\left(Y_{i}-\vec{g}\left(\hat{\Theta}_{k-1},\alpha\right)\right)^{t}\bar{\Sigma}_{d}^{\star-1}\left(Y_{i}-\vec{g}\left(\hat{\Theta}_{k-1},\alpha\right)\right),\\
\hat{\Theta}_{k} & =\left(n_{h}+n_{d}\right)^{-1}\left(\sum_{i\in\mathcal{H}}R_{i}+\sum_{i\in\mathcal{D}}g^{-1}\left(R_{i},\hat{\alpha}_{k}\right)\right).
\end{align}

\end_inset


\end_layout

\begin_layout Standard
The starting point of the algorithm, 
\begin_inset Formula $\hat{\Theta}_{0}$
\end_inset

 is calculated solely on the control subjects: 
\begin_inset Formula $\hat{\Theta}_{0}=\bar{R}_{h}$
\end_inset

, with 
\begin_inset Formula $\bar{R}_{h}=n_{h}^{-1}\sum_{i\in\mathcal{H}}R_{i}$
\end_inset

  the average correlation matrix of control subjects.
 Note that the value of 
\begin_inset Formula $T_{eff}$
\end_inset

 wasn't required during the optimization.
 
\end_layout

\begin_layout Subsection
Model Inference
\end_layout

\begin_layout Standard
The gradient of 
\begin_inset Formula $\mathcal{S}^{\left(d\right)}$
\end_inset

 is 
\begin_inset Formula $\mathbf{J}_{\alpha}g\cdot\tilde{\Sigma}_{d}^{\star-1}\sum_{i\in\mathcal{D}}\left(Y_{i}-\vec{g}\left(\Theta,\alpha\right)\right)$
\end_inset

 where 
\begin_inset Formula $\mathbf{J}_{\alpha}g$
\end_inset

 is the Jacobian matrix of 
\begin_inset Formula $\vec{g}\left(\Theta,\alpha\right)$
\end_inset

 by 
\begin_inset Formula $\alpha$
\end_inset

(
\begin_inset Formula $\left[\mathbf{J}_{\alpha}g\right]_{ij}=\frac{\partial\vec{g}_{i}\left(\Theta,\alpha\right)}{\partial\alpha_{j}}$
\end_inset

), and the gradient of 
\begin_inset Formula $\mathcal{S}^{\left(h\right)}$
\end_inset

 is defined similarly.
 By calculating the minima of 
\begin_inset Formula $\mathcal{S}$
\end_inset

, we are calculating the solution for 
\begin_inset Formula $\nabla_{\alpha}\mathcal{S}=\nabla_{\alpha}\left(\mathcal{S}^{\left(h\right)}+\mathcal{S}^{\left(d\right)}\right)=0$
\end_inset

 whereas 
\begin_inset Formula $E\left[\nabla_{\alpha}\mathcal{S}\right]=0$
\end_inset

 
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
do we need to prove this?
\end_layout

\end_inset

.
 Thus the minimization of 
\begin_inset Formula $\mathcal{S}$
\end_inset

 can be viewed as a solution of General Estimation Equations.
 Using the methodology of the latter
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
quote GEE paper
\end_layout

\end_inset

, we can use the lemma 
\begin_inset Formula $\sqrt{n}\cdot\left(\hat{\alpha}_{gee}-\alpha\right)\overset{d}{\longrightarrow}\mathcal{N}_{p}\left(0,V\left(\hat{\alpha}\right)\right)$
\end_inset

 to conduct inference on our estimates.
 In order to do so, we need to calculate 
\begin_inset Formula $V\left(\hat{\alpha}\right)$
\end_inset

, the variance estimate of 
\begin_inset Formula $\hat{\alpha}$
\end_inset

; 
\begin_inset Formula $V\left(\hat{\alpha}\right)$
\end_inset

 is calculated as a sandwich estimator 
\begin_inset Formula $V\left(\hat{\alpha}\right)=I^{\left(0\right)-1}I^{\left(1\right)}I^{\left(0\right)-1}$
\end_inset

 where 
\begin_inset Formula $I^{\left(0\right)}=I_{h}^{\left(0\right)}+I_{d}^{\left(0\right)}$
\end_inset

 (
\begin_inset Formula $I^{\left(1\right)}$
\end_inset

 defined similarly) .
 These, in turn, are defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
I_{d}^{\left(0\right)} & =n_{d}\cdot\mathbf{J}_{\alpha}g^{t}\cdot\bar{\Sigma}_{d}^{\star-1}\cdot\mathbf{J}_{\alpha}g|_{\alpha=\hat{\alpha}}\\
I_{d}^{\left(1\right)} & =n_{d}\cdot\mathbf{J}_{\alpha}g^{t}\cdot\bar{\Sigma}_{d}^{\star-1}\cdot C_{d}\cdot\bar{\Sigma}_{d}^{\star-1}\cdot\mathbf{J}_{\alpha}g|_{\alpha=\hat{\alpha}}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $C_{d}$
\end_inset

 is an empirical estimate of the covariance of 
\begin_inset Formula $Y_{i}$
\end_inset

 (
\begin_inset Formula $i\in\mathcal{D})$
\end_inset

, computed in the following way:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
C_{d}=n_{eff}^{-1}\sum_{i\in\mathcal{D}}\left(Y_{i}-\vec{g}\left(\Theta,\alpha\right)\right)\left(Y_{i}-\vec{g}\left(\Theta,\alpha\right)\right)^{t}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $I_{h}^{\left(0\right)},I_{h}^{\left(1\right)}$
\end_inset

 are defined similarly with 
\begin_inset Formula $\vec{\Theta}$
\end_inset

, 
\begin_inset Formula $\Sigma_{h}^{\star}$
\end_inset

 replacing 
\begin_inset Formula $\vec{g}\left(\Theta,\alpha\right)$
\end_inset

, 
\begin_inset Formula $\bar{\Sigma}_{d}^{\star}$
\end_inset

.
 In theory, 
\begin_inset Formula $\bar{\Sigma}_{d}^{\star}$
\end_inset

 should be divided by 
\begin_inset Formula $T_{eff}$
\end_inset

 in equations (),(); in practice 
\begin_inset Formula $T_{eff}$
\end_inset

 cancels out in the final multiplication of 
\begin_inset Formula $V\left(\hat{\alpha}\right)$
\end_inset

, and so it is omitted.
 This is the first appearance of 
\begin_inset Formula $n_{eff}$
\end_inset

: 
\begin_inset Formula $C_{d}$
\end_inset

 is the empirical covariance matrix of 
\begin_inset Formula $Y_{i}$
\end_inset

 based on 
\begin_inset Formula $n_{d}$
\end_inset

 independent subjects.
 However, the effective number of subjects is lower than 
\begin_inset Formula $n_{d}$
\end_inset

: 
\series bold

\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout

\series bold
WHY?
\end_layout

\end_inset

 we find that psi also fixes the bias introduced by ...
 [[can we show that n_eff ecreases when dim(alpha) increases?]]
\end_layout

\begin_layout Standard
We estimate 
\begin_inset Formula $n_{eff}$
\end_inset

 using Efron's RMS coefficient estimate 
\begin_inset Formula $\hat{\psi}_{d}=\sqrt{\sum_{i<j}\bar{R}_{d,ij}^{2}/{p \choose 2}}$
\end_inset

 and 
\begin_inset Formula $n_{eff}=\frac{n_{d}}{\left(n_{d}-1\right)\hat{\psi}_{d}^{2}+1}$
\end_inset

 ; Finally, we arrive with the following theorem, which in turn allows us
 to conduct inference on the estimates 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 with Z-tests.
\end_layout

\begin_layout Theorem*
The Limiting Distribution of Weighted Least Squares Estimates of Correlation
 Matrices
\end_layout

\begin_layout Theorem*
The estimates derived in chapter [[Model Estimation]] and the GEE Sandwich
 Estimates of their variance derived in chapter [[Model Inference]] have
 a limiting distribution of 
\end_layout

\begin_layout Theorem*
\begin_inset Formula 
\[
V\left(\hat{\alpha}\right)^{-\nicefrac{1}{2}}\left(\hat{\alpha}-\alpha\right)\overset{d}{\longrightarrow}N_{pq}\left(0,I_{pq}\right)
\]

\end_inset


\end_layout

\begin_layout Theorem*
and particularly, 
\begin_inset Formula $V\left(\hat{\alpha}\right)_{jj}^{-\nicefrac{1}{2}}\left(\hat{\alpha}_{j}-\alpha_{j}\right)\overset{d}{\longrightarrow}0\left(0,1\right)$
\end_inset

 element-wise.
 
\end_layout

\begin_layout Standard
The number of hypothesis tests is 
\begin_inset Formula $q$
\end_inset

 (the dimension of 
\begin_inset Formula $\alpha$
\end_inset

) that is controlled by the researcher; the selection of 
\begin_inset Formula $q$
\end_inset

 holds a trade-off between the flexibility of the model and the number of
 hypothesis to test.
 Setting a large 
\begin_inset Formula $q$
\end_inset

 eases the severity of the model assumptions on the data, while setting
 a low value minimizes the number of hypothesis.
 For example, by setting 
\begin_inset Formula $q=1$
\end_inset

, one has only 
\begin_inset Formula $p$
\end_inset

 comparisons, while the most simple model of comparing two averaged correlation
 matrices will end up with 
\begin_inset Formula $m\propto\nicefrac{p^{2}}{2}$
\end_inset

 comparisons.
 Increasing the number of hypothesis reduces the statistical power of the
 research, as a result of adjusting the p-values to control the FDR (or
 FWER); In that sense, our model's advantage is it's ability to preserve
 statistical power by reducing the number of comparisons.
 In our working example, we test the alternative hypothesis 
\begin_inset Formula $H_{\left(1\right),i}:\alpha_{i}<1$
\end_inset

 against the null 
\begin_inset Formula $H_{\left(0\right),i}:\alpha_{i}\geq1$
\end_inset

 (since 
\begin_inset Formula $\alpha=1$
\end_inset

 represents the null value of 
\begin_inset Formula $\alpha$
\end_inset

: 
\begin_inset Formula $\Theta_{ij}\cdot\alpha_{i}\alpha_{j}=\Theta_{ij}$
\end_inset

 for 
\begin_inset Formula $\alpha_{i},\alpha_{j}=1$
\end_inset

).
 We can then perform corrections to keep the FDR (or FWER) and FCR only
 on 
\begin_inset Formula $p$
\end_inset

 comparisons, as long as the procedure is robust to the covariance between
 estimates.
\end_layout

\begin_layout Subsection
Discussion
\end_layout

\begin_layout Standard
The estimates derived by the GEE solution vary from the ordinary least squares
 solution by the weighting matrix 
\begin_inset Formula $\tilde{\Sigma}^{\star-1}$
\end_inset

.
 OLS estimates and inference will yield misspecified variance estimates
 since the columns of 
\begin_inset Formula $Y_{i}$
\end_inset

 aren't independent.
 The GEE method minimizes a weighted least squares equation where distances
 are inversely weighted by their covariance - in our case, we we use the
 covariance structure of 
\begin_inset Formula $Y_{i}$
\end_inset

 defined in lemma ().
 
\end_layout

\begin_layout Standard
Regarding the estimation of 
\begin_inset Formula $\hat{\Theta}$
\end_inset

, one would suggest using 
\begin_inset Formula $\bar{R}_{h}=n_{h}^{-1}\sum_{i\in\mathcal{H}}R_{i}$
\end_inset

 as it is an unbiased estimator of 
\begin_inset Formula $\Theta$
\end_inset

 (IS IT?); however this estimator isn't efficient - it ignores information
 that can be gained from diagnosed subjects.
  An efficient estimator of 
\begin_inset Formula $\Theta$
\end_inset

 must use the information gained by both sets.
 Since 
\begin_inset Formula $\hat{\Theta}$
\end_inset

 isn't a linear estimator of 
\begin_inset Formula $\Theta$
\end_inset

, we introduce more bias relative to a simple average of the control subjects;
 However the integration of the 
\begin_inset Formula $n_{d}$
\end_inset

 diagnosed subjects increases it's efficiency.
 
\end_layout

\begin_layout Standard
Another note is the sandwich property of 
\begin_inset Formula $V\left(\hat{\alpha}\right)$
\end_inset

.
 Under theoretical assumptions, both 
\begin_inset Formula $\nicefrac{\Sigma_{\bar{R}_{d}}^{\star}}{T_{eff}},\widehat{Cov}_{D}\overset{p}{\longrightarrow}\nicefrac{\Sigma_{\Lambda_{d}}^{\star}}{T_{eff}}$
\end_inset

, yielding 
\begin_inset Formula $\Sigma_{\bar{R}_{d}}^{\star-1}\cdot\widehat{Cov}_{D}\cdot\Sigma_{\bar{R}_{h}}^{\star-1}\overset{p}{\longrightarrow}T_{eff}\cdot\Sigma_{\Lambda_{d}}^{\star}$
\end_inset

.
 The usage of the sandwich estimator helps keep the type-I error even when
 asymptotic assumptions fail to hold.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\hat{\psi}_{d}$
\end_inset

 is over-estimating the effect of row-wise correlation on the effective
 degrees of freedom 
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
quote Efron
\end_layout

\end_inset

 and causes an over-estimation of the variance of 
\begin_inset Formula $\hat{\alpha}$
\end_inset

, resulting with slightly decreased statistical power.
 Although not an optimal solution, this is a better option than failing
 to control the Type-I error that results with under-estimating the variance.
 
\end_layout

\begin_layout Section
Simulations
\end_layout

\begin_layout Algorithm
\begin_inset Float algorithm
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Generate Sample Parameters
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\series bold
Parameters:
\series default
 
\begin_inset Formula $p$
\end_inset

, 
\begin_inset Formula $\alpha_{prop}$
\end_inset

, 
\begin_inset Formula $\alpha_{range}$
\end_inset


\end_layout

\begin_layout Enumerate
Generate 
\begin_inset Formula $\Theta$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Enumerate
Generate 
\begin_inset Formula $x\sim\mathcal{MN}_{2p\times p}\left(0,I_{2p},I_{p}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $x\leftarrow x^{t}x$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\Theta\leftarrow diag\left(x\right)^{-\nicefrac{1}{2}}\cdot x\cdot diag\left(x\right)^{-\nicefrac{1}{2}}$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Generate 
\begin_inset Formula $\alpha$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Enumerate
Generate 
\begin_inset Formula $\alpha_{j}\overset{iid}{\sim}Unif\left(\min\left(\alpha_{range}\right),\max\left(\alpha_{range}\right)\right)$
\end_inset

, 
\begin_inset Formula $j=1,...,\alpha_{prop}\cdot p$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\alpha_{p\cdot\alpha_{prop}+1},...,\alpha_{p}\leftarrow1$
\end_inset


\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Algorithm
\begin_inset Float algorithm
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Generate ARMA-Based Covariance Martrix
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\series bold
Parameters:
\series default
 
\begin_inset Formula $n\in\mathbb{N}$
\end_inset

, 
\begin_inset Formula $V\in\mathbb{M}_{d}\left(\mathbb{R}\right)$
\end_inset

, 
\begin_inset Formula $a\in\mathbb{R}^{p}$
\end_inset

, 
\begin_inset Formula $m\in\mathbb{R}^{q}$
\end_inset


\end_layout

\begin_layout Enumerate
Generate 
\begin_inset Formula $X\sim\mathcal{MN}_{n\times d}\left(0,I_{n},V\right)$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $Y\leftarrow X$
\end_inset


\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $1<i\leq n$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $Y_{i,\cdot}\leftarrow\sum_{j=1}^{q}a_{j}X_{i-j,\cdot}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $Y_{i,\cdot}\leftarrow\sum_{j=1}^{p}m_{j}Y_{i-j,\cdot}$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset Formula $Y\leftarrow Y^{t}Y$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Algorithm
\begin_inset Float algorithm
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Generate Samples
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\series bold
Parameters:
\series default
 
\begin_inset Formula $\Lambda,$
\end_inset


\begin_inset Formula $n$
\end_inset

, 
\begin_inset Formula $T$
\end_inset

, 
\begin_inset Formula $a$
\end_inset

, 
\begin_inset Formula $m$
\end_inset


\end_layout

\begin_layout Enumerate
Define 
\begin_inset Formula $S\in\mathbb{M}_{p}\left(\mathbb{R}\right)$
\end_inset

, 
\begin_inset Formula $S_{jj}\overset{iid}{\sim}Unif\left(\sqrt{10},10\right)$
\end_inset

, 
\begin_inset Formula $S_{ij}=0\forall i\neq j$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\Xi\leftarrow S\cdot\Lambda\cdot S$
\end_inset


\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $i\in\left\{ 1,...,n\right\} $
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Enumerate
If 
\begin_inset Formula $a,m$
\end_inset

 are empty:
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $X_{i}\sim Wishart_{p}\left(\Lambda,T\right)$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $X_{i}\leftarrow diag\left(X_{i}\right)^{-\nicefrac{1}{2}}\cdot X_{i}\cdot diag\left(X_{i}\right)^{-\nicefrac{1}{2}}$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Else:
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $X_{i}\leftarrow Algorithm\;2$
\end_inset


\end_layout

\end_deeper
\end_deeper
\end_inset


\end_layout

\begin_layout Algorithm
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Bias of 
\begin_inset Formula $\hat{\Theta},\hat{\alpha}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/itama/Google Drive/Documents/Study/05 Year 2018-2019/74445 Masters Thesis/correlation_glm/main_work/simulations/bias_small_sample.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
Not Bias - Estimate vs real
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
Instead of boxplot - have less null alphas
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Algorithm
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Bias of 
\begin_inset Formula $\hat{\Theta},\hat{\alpha}$
\end_inset

 as a Function of 
\begin_inset Formula $n,p$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/itama/Google Drive/Documents/Study/05 Year 2018-2019/74445 Masters Thesis/correlation_glm/main_work/simulations/bias_function_n.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
Narrower Boxplots - without dots
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
More sample size
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
Show only alpha
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
Show RMSE also
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
For p - do group by (with fill) instead of facet grid
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
two rows - the first is bias, the second is RMSE.
 No Theta, P with aes(fill=p) instaed of facet_grid
\end_layout

\begin_layout Plain Layout
Write that ARMA doesn't matter - maybe plot but don't show
\end_layout

\end_inset


\end_layout

\begin_layout Algorithm

\end_layout

\begin_layout Algorithm
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Consistency of 
\begin_inset Formula $\hat{V}\left(\hat{\alpha}\right)$
\end_inset

 over 
\begin_inset Formula $n,p$
\end_inset

, Under Strong Null and No Time Dependency
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Algorithm
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Consistency of 
\begin_inset Formula $\hat{V}\left(\hat{\alpha}\right)$
\end_inset

 over 
\begin_inset Formula $n,p$
\end_inset

, Under Weak Null and Time Dependency
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Effect of 
\begin_inset Formula $\frac{n_{d}}{n_{h}+n_{d}}$
\end_inset

 on
\begin_inset Formula $\hat{V}\left(\text{\hat{\alpha}}\right)$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/itama/Google Drive/Documents/Study/05 Year 2018-2019/74445 Masters Thesis/correlation_glm/main_work/simulations/percent_sick_effect.png
	scale 50

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this chapter, we will present the model's performance on simulated data.
 We will start by simulating data based on the model's assumptions - the
 control and diagnosed subjects have an expected correlation matrix of 
\begin_inset Formula $\Theta$
\end_inset

 and 
\begin_inset Formula $g\left(\Theta,\alpha\right)$
\end_inset

, respectivaly, each matrix based on 
\begin_inset Formula $T$
\end_inset

 independent multivariate measurements.
 We will then relax the assumptions of independent measurements by simulating
 correlation matrices based on time-dependent measurements.
 We will also present robustness simulations - simulating from one link
 function and estimating using another, and simulating from two random correlati
on matrices (with no link function).
 We will also present the model's ability to keep the FWER, Power simulations
 and comparisons, and calculation times.
 We present the algorithms to simulate the data in [[ref algorithms]].
 
\end_layout

\begin_layout Standard
The first simulations show the bias of the estimates 
\begin_inset Formula $\hat{\Theta},\hat{\alpha}$
\end_inset

.
 In plot [[ref]], we simulate a random sample of 50 diagnosed and control
 subjects each, with 
\begin_inset Formula $p=50$
\end_inset

.
 40% of the 
\begin_inset Formula $\alpha$
\end_inset

 values are uniformilly distributed between 
\begin_inset Formula $\left(0.7,1.1\right)$
\end_inset

 whilst the other are equal to 1.
 The estimates are unbiased for both parameters.
 In plot [[ref]], we allow the sample size and the dimension 
\begin_inset Formula $p$
\end_inset

 to vary - the sample size varies from 20 to 100 while the simension varies
 from 10 to 40.
 
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
talk about other parameters
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the next simulations, we compare the estimated GEE variance to the bootstrape
d empirical variance - for each 
\begin_inset Formula $\Theta,\alpha$
\end_inset

 we simulate 100 samples, each holding a sample size of 
\begin_inset Formula $n_{h}+n_{d}$
\end_inset

.
 Then, we estimate the empirical variance of 
\begin_inset Formula $\hat{\Theta},\hat{\alpha}$
\end_inset

 and compare it the average GEE variance estimate, computed in each simulation.
 In plot [[ref]], we hold the the proportion of diagnosed subjects (
\begin_inset Formula $\frac{n_{d}}{n_{d}+n_{h}}$
\end_inset

) constant, and allow 
\begin_inset Formula $n$
\end_inset

 and 
\begin_inset Formula $p$
\end_inset

 to vary.
 We show that the variance estimate is consistent, for null cases (all 
\begin_inset Formula $\alpha$
\end_inset

-s equal to 1) with no time-dependence, and for non-null cases (some 
\begin_inset Formula $\alpha$
\end_inset

-s don't equal to 1) with time-dependence as well.
 In plot [[ref]], we set 
\begin_inset Formula $n=50$
\end_inset

 and 
\begin_inset Formula $p=25$
\end_inset

, but allow the proportion 
\begin_inset Formula $\frac{n_{d}}{n_{d}+n_{h}}$
\end_inset

 to vary.
 Interestingly, but somewhat intuitivly, we find that the GEE estimate tends
 to be lower when the proportion is low, in reference to the bootstrapped
 variance.
 However - the GEE estimate always overestimates the variance, keeping the
 FWER under strong control.
 
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
explain why
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
We further show 
\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
calculation times
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
FWER & FDR
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Power: effect size 
\begin_inset Formula $\alpha$
\end_inset

, number of non-null, n VS power
\end_layout

\end_inset

for every sample, mean(p<.05) by is_null -> alot of samples (to show power
 + FWER/FDR).
 
\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Power: Comparison - for T statisics - atleast one element in column is significa
nt
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
robustness - with simulation link function
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
robustness - without simulation link : 2 different matrices and difference
 one or 2 columns
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
robustness - Difference between 
\begin_inset Formula $\bar{R}_{d}$
\end_inset

 and 
\begin_inset Formula $g\left(\hat{\Theta},\hat{\alpha}\right)$
\end_inset

, diagnostics?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Write the text for these simulations:
\end_layout

\begin_layout Enumerate
Randomize from Random Effects - does n_eff (and rms alpha) change?
\end_layout

\begin_layout Enumerate
Does n_eff vary when ARMA is different??
\end_layout

\begin_layout Enumerate
Does n_eff ecreases when dim(alpha) increases?
\end_layout

\begin_layout Section
Estimate on Real Data
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
g_{ij}=\frac{\Theta_{ij}}{\left(1+\alpha_{i}+\alpha_{j}\right)}
\]

\end_inset


\end_layout

\begin_layout Standard
try constraints on 
\begin_inset Formula $\alpha$
\end_inset

?
\end_layout

\end_body
\end_document
